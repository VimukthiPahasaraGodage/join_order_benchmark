{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1cdb47-2c49-4c1e-b452-42e98c5f4433",
   "metadata": {},
   "source": [
    "# aka_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1f0524-ea11-41d4-8759-c65635a703e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing aka_title.csv for table aka_title\n",
      "Original dataset shape:  (361377, 12)\n",
      "Cleaned dataset shape:  (361377, 12)\n",
      "Saved cleaned CSV to cleaned\\aka_title.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Define your schema ---\n",
    "SCHEMA = {\n",
    "    \"aka_title\": [\n",
    "        (\"id\", int, True),\n",
    "        (\"movie_id\", int, True),\n",
    "        (\"title\", str, False),\n",
    "        (\"imdb_index\", str, False),\n",
    "        (\"kind_id\", int, True),\n",
    "        (\"production_year\", int, False),  # Can be null\n",
    "        (\"phonetic_code\", str, False),\n",
    "        (\"episode_of_id\", int, False),\n",
    "        (\"season_nr\", int, False),\n",
    "        (\"episode_nr\", int, False),\n",
    "        (\"note\", str, False),\n",
    "        (\"md5sum\", str, False)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Utility function to clean a single CSV ---\n",
    "def clean_csv(csv_path, table_name, output_dir):\n",
    "    print(f\"Processing {csv_path} for table {table_name}\")\n",
    "    \n",
    "    schema = SCHEMA[table_name]\n",
    "    required_columns = [col for col, _, _ in schema]\n",
    "    types = {col: dtype for col, dtype, _ in schema}\n",
    "    not_null_cols = {col for col, _, not_null in schema if not_null}\n",
    "\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    print(\"Original dataset shape: \", df.shape)\n",
    "\n",
    "    # Keep only needed columns\n",
    "    df = df.iloc[:, :len(required_columns)]\n",
    "    df.columns = required_columns\n",
    "\n",
    "    # Enforce types\n",
    "    for col, dtype in types.items():\n",
    "        try:\n",
    "            if dtype == int:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').dropna().astype('Int64')\n",
    "            elif dtype == float:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else:\n",
    "                df[col] = df[col].astype(str).where(df[col].notnull(), None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {col} in {table_name}: {e}\")\n",
    "\n",
    "    # Drop rows with NULLs in NOT NULL columns\n",
    "    df.dropna(subset=not_null_cols, inplace=True)\n",
    "\n",
    "    # Save cleaned file\n",
    "    output_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "    df.to_csv(output_path, index=False, header=False)\n",
    "    print(\"Cleaned dataset shape: \", df.shape)\n",
    "    print(f\"Saved cleaned CSV to {output_path}\\n\")\n",
    "\n",
    "# --- Main script ---\n",
    "def clean_all_csvs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for table_name in SCHEMA.keys():\n",
    "        csv_path = os.path.join(input_dir, f\"{table_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            clean_csv(csv_path, table_name, output_dir)\n",
    "        else:\n",
    "            print(f\"Warning: CSV file for table {table_name} not found.\")\n",
    "\n",
    "# --- Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cleaned\", exist_ok=True)\n",
    "    clean_csv(\"aka_title.csv\", \"aka_title\", \"cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe9d0d-7940-4bf2-b06b-b904dd37b100",
   "metadata": {},
   "source": [
    "# cast_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d844f7bf-e6da-4bbe-a5c9-b4ff17b8764c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cast_info.csv for table cast_info\n",
      "Original dataset shape:  (36237143, 7)\n",
      "Cleaned dataset shape:  (36237143, 7)\n",
      "Saved cleaned CSV to cleaned\\cast_info.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Define your schema ---\n",
    "SCHEMA = {\n",
    "    \"cast_info\": [\n",
    "        (\"id\", int, True),\n",
    "        (\"person_id\", int, True),\n",
    "        (\"movie_id\", int, True),\n",
    "        (\"person_role_id\", int, False),\n",
    "        (\"note\", str, False),\n",
    "        (\"nr_order\", int, False),  # Can be null\n",
    "        (\"role_id\", int, True)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Utility function to clean a single CSV ---\n",
    "def clean_csv(csv_path, table_name, output_dir):\n",
    "    print(f\"Processing {csv_path} for table {table_name}\")\n",
    "    \n",
    "    schema = SCHEMA[table_name]\n",
    "    required_columns = [col for col, _, _ in schema]\n",
    "    types = {col: dtype for col, dtype, _ in schema}\n",
    "    not_null_cols = {col for col, _, not_null in schema if not_null}\n",
    "\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    print(\"Original dataset shape: \", df.shape)\n",
    "\n",
    "    # Keep only needed columns\n",
    "    df = df.iloc[:, :len(required_columns)]\n",
    "    df.columns = required_columns\n",
    "\n",
    "    # Enforce types\n",
    "    for col, dtype in types.items():\n",
    "        try:\n",
    "            if dtype == int:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').dropna().astype('Int64')\n",
    "            elif dtype == float:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else:\n",
    "                df[col] = df[col].astype(str).where(df[col].notnull(), None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {col} in {table_name}: {e}\")\n",
    "\n",
    "    # Drop rows with NULLs in NOT NULL columns\n",
    "    df.dropna(subset=not_null_cols, inplace=True)\n",
    "\n",
    "    # Save cleaned file\n",
    "    output_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "    df.to_csv(output_path, index=False, header=False)\n",
    "    print(\"Cleaned dataset shape: \", df.shape)\n",
    "    print(f\"Saved cleaned CSV to {output_path}\\n\")\n",
    "\n",
    "# --- Main script ---\n",
    "def clean_all_csvs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for table_name in SCHEMA.keys():\n",
    "        csv_path = os.path.join(input_dir, f\"{table_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            clean_csv(csv_path, table_name, output_dir)\n",
    "        else:\n",
    "            print(f\"Warning: CSV file for table {table_name} not found.\")\n",
    "\n",
    "# --- Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cleaned\", exist_ok=True)\n",
    "    clean_csv(\"cast_info.csv\", \"cast_info\", \"cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d4990-8044-479a-9ea6-0cc415eaae14",
   "metadata": {},
   "source": [
    "# char_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf3a83da-3976-4b44-ac20-e7f60e44bceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing char_name.csv for table char_name\n",
      "Original dataset shape:  (3139686, 7)\n",
      "Cleaned dataset shape:  (3139685, 7)\n",
      "Saved cleaned CSV to cleaned\\char_name.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Define your schema ---\n",
    "SCHEMA = {\n",
    "    \"char_name\": [\n",
    "        (\"id\", int, True),\n",
    "        (\"name\", str, True),\n",
    "        (\"imdb_index\", str, False),\n",
    "        (\"imdb_id\", int, False),\n",
    "        (\"name_pcode_nf\", str, False),\n",
    "        (\"surname_pcode\", str, False),  # Can be null\n",
    "        (\"md5sum\", str, False)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Utility function to clean a single CSV ---\n",
    "def clean_csv(csv_path, table_name, output_dir):\n",
    "    print(f\"Processing {csv_path} for table {table_name}\")\n",
    "    \n",
    "    schema = SCHEMA[table_name]\n",
    "    required_columns = [col for col, _, _ in schema]\n",
    "    types = {col: dtype for col, dtype, _ in schema}\n",
    "    not_null_cols = {col for col, _, not_null in schema if not_null}\n",
    "\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_path, header=None, low_memory=False)\n",
    "    print(\"Original dataset shape: \", df.shape)\n",
    "\n",
    "    # Keep only needed columns\n",
    "    df = df.iloc[:, :len(required_columns)]\n",
    "    df.columns = required_columns\n",
    "\n",
    "    # Enforce types\n",
    "    for col, dtype in types.items():\n",
    "        try:\n",
    "            if dtype == int:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').dropna().astype('Int64')\n",
    "            elif dtype == float:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else:\n",
    "                df[col] = df[col].astype(str).where(df[col].notnull(), None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {col} in {table_name}: {e}\")\n",
    "\n",
    "    # Drop rows with NULLs in NOT NULL columns\n",
    "    df.dropna(subset=not_null_cols, inplace=True)\n",
    "\n",
    "    # Save cleaned file\n",
    "    output_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "    df.to_csv(output_path, index=False, header=False)\n",
    "    print(\"Cleaned dataset shape: \", df.shape)\n",
    "    print(f\"Saved cleaned CSV to {output_path}\\n\")\n",
    "\n",
    "# --- Main script ---\n",
    "def clean_all_csvs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for table_name in SCHEMA.keys():\n",
    "        csv_path = os.path.join(input_dir, f\"{table_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            clean_csv(csv_path, table_name, output_dir)\n",
    "        else:\n",
    "            print(f\"Warning: CSV file for table {table_name} not found.\")\n",
    "\n",
    "# --- Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cleaned\", exist_ok=True)\n",
    "    clean_csv(\"char_name.csv\", \"char_name\", \"cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd118d-b6c5-405f-bde4-4f931f3f954a",
   "metadata": {},
   "source": [
    "# company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aef5903a-edcc-4bfa-9162-cbeaf1e0ec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing company_name.csv for table company_name\n",
      "Original dataset shape:  (234994, 7)\n",
      "Cleaned dataset shape:  (234992, 7)\n",
      "Saved cleaned CSV to cleaned\\company_name.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Define your schema ---\n",
    "SCHEMA = {\n",
    "    \"company_name\": [\n",
    "        (\"id\", int, True),\n",
    "        (\"name\", str, True),\n",
    "        (\"country_code\", str, False),\n",
    "        (\"imdb_id\", int, False),\n",
    "        (\"name_pcode_nf\", str, False),\n",
    "        (\"name_pcode_sf\", str, False),  # Can be null\n",
    "        (\"md5sum\", str, False)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Utility function to clean a single CSV ---\n",
    "def clean_csv(csv_path, table_name, output_dir):\n",
    "    print(f\"Processing {csv_path} for table {table_name}\")\n",
    "    \n",
    "    schema = SCHEMA[table_name]\n",
    "    required_columns = [col for col, _, _ in schema]\n",
    "    types = {col: dtype for col, dtype, _ in schema}\n",
    "    not_null_cols = {col for col, _, not_null in schema if not_null}\n",
    "\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_path, header=None, low_memory=False)\n",
    "    print(\"Original dataset shape: \", df.shape)\n",
    "\n",
    "    # Keep only needed columns\n",
    "    df = df.iloc[:, :len(required_columns)]\n",
    "    df.columns = required_columns\n",
    "\n",
    "    # Enforce types\n",
    "    for col, dtype in types.items():\n",
    "        try:\n",
    "            if dtype == int:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').dropna().astype('Int64')\n",
    "            elif dtype == float:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else:\n",
    "                df[col] = df[col].astype(str).where(df[col].notnull(), None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {col} in {table_name}: {e}\")\n",
    "\n",
    "    # Drop rows with NULLs in NOT NULL columns\n",
    "    df.dropna(subset=not_null_cols, inplace=True)\n",
    "\n",
    "    # Save cleaned file\n",
    "    output_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "    df.to_csv(output_path, index=False, header=False)\n",
    "    print(\"Cleaned dataset shape: \", df.shape)\n",
    "    print(f\"Saved cleaned CSV to {output_path}\\n\")\n",
    "\n",
    "# --- Main script ---\n",
    "def clean_all_csvs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for table_name in SCHEMA.keys():\n",
    "        csv_path = os.path.join(input_dir, f\"{table_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            clean_csv(csv_path, table_name, output_dir)\n",
    "        else:\n",
    "            print(f\"Warning: CSV file for table {table_name} not found.\")\n",
    "\n",
    "# --- Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cleaned\", exist_ok=True)\n",
    "    clean_csv(\"company_name.csv\", \"company_name\", \"cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd0ddea-d154-41b2-8b44-d765c6d9e427",
   "metadata": {},
   "source": [
    "# movie_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a39c2284-95e8-4eff-b3d7-86b1ae627a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing movie_info.csv for table movie_info\n",
      "Original dataset shape:  (14504446, 5)\n",
      "Cleaned dataset shape:  (14499888, 5)\n",
      "Saved cleaned CSV to cleaned\\movie_info.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Define your schema ---\n",
    "SCHEMA = {\n",
    "    \"movie_info\": [\n",
    "        (\"id\", int, True),\n",
    "        (\"movie_id\", int, True),\n",
    "        (\"info_type_id\", int, True),\n",
    "        (\"info\", str, True),\n",
    "        (\"note\", str, False)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Utility function to clean a single CSV ---\n",
    "def clean_csv(csv_path, table_name, output_dir):\n",
    "    print(f\"Processing {csv_path} for table {table_name}\")\n",
    "    \n",
    "    schema = SCHEMA[table_name]\n",
    "    required_columns = [col for col, _, _ in schema]\n",
    "    types = {col: dtype for col, dtype, _ in schema}\n",
    "    not_null_cols = {col for col, _, not_null in schema if not_null}\n",
    "\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_path, header=None, low_memory=False)\n",
    "    print(\"Original dataset shape: \", df.shape)\n",
    "\n",
    "    # Keep only needed columns\n",
    "    df = df.iloc[:, :len(required_columns)]\n",
    "    df.columns = required_columns\n",
    "\n",
    "    # Enforce types\n",
    "    for col, dtype in types.items():\n",
    "        try:\n",
    "            if dtype == int:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').dropna().astype('Int64')\n",
    "            elif dtype == float:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else:\n",
    "                df[col] = df[col].astype(str).where(df[col].notnull(), None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {col} in {table_name}: {e}\")\n",
    "\n",
    "    # Drop rows with NULLs in NOT NULL columns\n",
    "    df.dropna(subset=not_null_cols, inplace=True)\n",
    "\n",
    "    # Save cleaned file\n",
    "    output_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "    df.to_csv(output_path, index=False, header=False)\n",
    "    print(\"Cleaned dataset shape: \", df.shape)\n",
    "    print(f\"Saved cleaned CSV to {output_path}\\n\")\n",
    "\n",
    "# --- Main script ---\n",
    "def clean_all_csvs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for table_name in SCHEMA.keys():\n",
    "        csv_path = os.path.join(input_dir, f\"{table_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            clean_csv(csv_path, table_name, output_dir)\n",
    "        else:\n",
    "            print(f\"Warning: CSV file for table {table_name} not found.\")\n",
    "\n",
    "# --- Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cleaned\", exist_ok=True)\n",
    "    clean_csv(\"movie_info.csv\", \"movie_info\", \"cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648957bd-4bc4-435d-8092-9298c0215fcf",
   "metadata": {},
   "source": [
    "# person_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d528af3-b4e2-465f-81f7-234e9e2ba26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing person_info.csv for table person_info\n",
      "Original dataset shape:  (2340492, 5)\n",
      "Cleaned dataset shape:  (2340491, 5)\n",
      "Saved cleaned CSV to cleaned\\person_info.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Define your schema ---\n",
    "SCHEMA = {\n",
    "    \"person_info\": [\n",
    "        (\"id\", int, True),\n",
    "        (\"person_id\", int, True),\n",
    "        (\"info_type_id\", int, True),\n",
    "        (\"info\", str, True),\n",
    "        (\"note\", str, False)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Utility function to clean a single CSV ---\n",
    "def clean_csv(csv_path, table_name, output_dir):\n",
    "    print(f\"Processing {csv_path} for table {table_name}\")\n",
    "    \n",
    "    schema = SCHEMA[table_name]\n",
    "    required_columns = [col for col, _, _ in schema]\n",
    "    types = {col: dtype for col, dtype, _ in schema}\n",
    "    not_null_cols = {col for col, _, not_null in schema if not_null}\n",
    "\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_path, header=None, low_memory=False)\n",
    "    print(\"Original dataset shape: \", df.shape)\n",
    "\n",
    "    # Keep only needed columns\n",
    "    df = df.iloc[:, :len(required_columns)]\n",
    "    df.columns = required_columns\n",
    "\n",
    "    # Enforce types\n",
    "    for col, dtype in types.items():\n",
    "        try:\n",
    "            if dtype == int:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').dropna().astype('Int64')\n",
    "            elif dtype == float:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else:\n",
    "                df[col] = df[col].astype(str).where(df[col].notnull(), None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {col} in {table_name}: {e}\")\n",
    "\n",
    "    # Drop rows with NULLs in NOT NULL columns\n",
    "    df.dropna(subset=not_null_cols, inplace=True)\n",
    "\n",
    "    # Save cleaned file\n",
    "    output_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "    df.to_csv(output_path, index=False, header=False)\n",
    "    print(\"Cleaned dataset shape: \", df.shape)\n",
    "    print(f\"Saved cleaned CSV to {output_path}\\n\")\n",
    "\n",
    "# --- Main script ---\n",
    "def clean_all_csvs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for table_name in SCHEMA.keys():\n",
    "        csv_path = os.path.join(input_dir, f\"{table_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            clean_csv(csv_path, table_name, output_dir)\n",
    "        else:\n",
    "            print(f\"Warning: CSV file for table {table_name} not found.\")\n",
    "\n",
    "# --- Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cleaned\", exist_ok=True)\n",
    "    clean_csv(\"person_info.csv\", \"person_info\", \"cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a565de-e6d1-46af-84d4-fe5d88bff024",
   "metadata": {},
   "source": [
    "# title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ae676a2-42bc-4f13-87e6-a54018b65514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing title.csv for table title\n",
      "Original dataset shape:  (2527952, 12)\n",
      "Cleaned dataset shape:  (2527950, 12)\n",
      "Saved cleaned CSV to cleaned\\title.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Define your schema ---\n",
    "SCHEMA = {\n",
    "    \"title\": [\n",
    "        (\"id\", int, True),\n",
    "        (\"title\", str, True),\n",
    "        (\"imdb_index\", str, False),\n",
    "        (\"kind_id\", int, True),\n",
    "        (\"production_year\", int, False),\n",
    "        (\"imdb_id\", int, False),\n",
    "        (\"phonetic_code\", str, False),\n",
    "        (\"episode_of_id\", int, False),\n",
    "        (\"season_nr\", int, False),\n",
    "        (\"episode_nr\", int, False),\n",
    "        (\"series_years\", str, False),\n",
    "        (\"md5sum\", str, False),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Utility function to clean a single CSV ---\n",
    "def clean_csv(csv_path, table_name, output_dir):\n",
    "    print(f\"Processing {csv_path} for table {table_name}\")\n",
    "    \n",
    "    schema = SCHEMA[table_name]\n",
    "    required_columns = [col for col, _, _ in schema]\n",
    "    types = {col: dtype for col, dtype, _ in schema}\n",
    "    not_null_cols = {col for col, _, not_null in schema if not_null}\n",
    "\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_path, header=None, low_memory=False)\n",
    "    print(\"Original dataset shape: \", df.shape)\n",
    "\n",
    "    # Keep only needed columns\n",
    "    df = df.iloc[:, :len(required_columns)]\n",
    "    df.columns = required_columns\n",
    "\n",
    "    # Enforce types\n",
    "    for col, dtype in types.items():\n",
    "        try:\n",
    "            if dtype == int:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').dropna().astype('Int64')\n",
    "            elif dtype == float:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else:\n",
    "                df[col] = df[col].astype(str).where(df[col].notnull(), None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {col} in {table_name}: {e}\")\n",
    "\n",
    "    # Drop rows with NULLs in NOT NULL columns\n",
    "    df.dropna(subset=not_null_cols, inplace=True)\n",
    "\n",
    "    # Save cleaned file\n",
    "    output_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "    df.to_csv(output_path, index=False, header=False)\n",
    "    print(\"Cleaned dataset shape: \", df.shape)\n",
    "    print(f\"Saved cleaned CSV to {output_path}\\n\")\n",
    "\n",
    "# --- Main script ---\n",
    "def clean_all_csvs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for table_name in SCHEMA.keys():\n",
    "        csv_path = os.path.join(input_dir, f\"{table_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            clean_csv(csv_path, table_name, output_dir)\n",
    "        else:\n",
    "            print(f\"Warning: CSV file for table {table_name} not found.\")\n",
    "\n",
    "# --- Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cleaned\", exist_ok=True)\n",
    "    clean_csv(\"title.csv\", \"title\", \"cleaned\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
